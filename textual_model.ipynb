{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, GlobalMaxPooling1D, Bidirectional\n",
    "from keras.layers import Dropout, GRU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics.pairwise import cosine_similarity,euclidean_distances\n",
    "import datetime\n",
    "from IPython.core.interactiveshell import InteractiveShell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape =(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape =(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "cleaned_text_path  = \"*/cleaned-text/\"\n",
    "entries = []\n",
    "filenames =[]\n",
    "for i in range(64):\n",
    "    for file_name in glob.glob(cleaned_text_path+str(i+1)+\"/*.cleaned.txt\"):\n",
    "        print(file_name)\n",
    "        filenames.append(file_name)\n",
    "        entries.append({\n",
    "            \"content\":open(file_name, encoding='utf-8').read(),\n",
    "            \"label\":str(i+1),\n",
    "            \"filepaths\":file_name,\n",
    "            \"manuscriptID\":str(i+1),\n",
    "        })\n",
    "filenames = np.array(filenames)\n",
    "df = pd.DataFrame(entries)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', \n",
    "                      lower=True, oov_token=\"UNK\")\n",
    "\n",
    "tokenizer.fit_on_texts(df['content'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code with attention\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, \n",
    "                    100,  \n",
    "                    input_length=MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(64, dropout=0.5, recurrent_dropout=0.5, return_sequences=True)))\n",
    "model.add(Attention(MAX_SEQUENCE_LENGTH))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='softmax'))\n",
    "model.compile(loss='weighted_categorical_crossentropy', optimizer=Adam(learning_rate=0.000001), metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(df['content'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
    "Y = pd.get_dummies(df['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "\n",
    "new_model = Model(model.inputs, model.layers[-2].output)\n",
    "new_model.compile(optimizer='Adam', loss='weighted_categorical_crossentropy', metrics=['accuracy'])\n",
    "new_model.summary()\n",
    "X_feature = new_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10 \n",
    "start_time = datetime.datetime.now().timestamp()\n",
    "\n",
    "total_accuracy = 0\n",
    "Y_pred_list = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "for key,image_path in df['filepaths'].iteritems():\n",
    "    #Get the predicted feature vector for the given image\n",
    "    content = open(image_path, encoding='utf-8').read()\n",
    "    X_Q = tokenizer.texts_to_sequences([content])\n",
    "    X_Q = pad_sequences(X_Q, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
    "    pred_feature_vec =  new_model.predict(X_Q) \n",
    "    \n",
    "    #Find the cosine similarity array based on all the feature vectors \n",
    "    #stored in X\n",
    "    similarity_array = cosine_similarity(pred_feature_vec,X_feature)[0]\n",
    "    \n",
    "    #Get top K indices \n",
    "    indices = similarity_array.argsort()[-K:][::-1]\n",
    "    \n",
    "    true_ID = df['manuscriptID'].loc[key]\n",
    "    total_pages = df[ df['manuscriptID'] == true_ID]['filepaths'].count()\n",
    "    predicted_arr = df['manuscriptID'].loc[indices].values\n",
    "    \n",
    "    Y_pred_list.append(predicted_arr[0])\n",
    "    \n",
    "    #Number of correct predictions out of K\n",
    "    found = np.count_nonzero(predicted_arr == true_ID)\n",
    "    #print(indices)    \n",
    "    if total_pages >= K:\n",
    "        total_accuracy +=  found/K\n",
    "    else:\n",
    "        #total_pages is less than K\n",
    "        total_accuracy += found/total_pages\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    if count > 0 and count % 100 == 0:\n",
    "        print(\"Done \", count)\n",
    "        print(\"Accuracy so far %g %%\" % (total_accuracy/count * 100))\n",
    "       \n",
    "end_time =  datetime.datetime.now().timestamp()\n",
    "total_retrieval_time = end_time - start_time #In Seconds\n",
    "print(\"Total retrieval time %g seconds\" % total_retrieval_time)\n",
    "\n",
    "mean_accuracy = total_accuracy/df.shape[0]\n",
    "print(\"\\nThe mean accuracy for top %d images is %g %%\" % (K, mean_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text_file  = \"*/cleaned-text/2/DSC00009.JPG.txt.cleaned.txt\"\n",
    "content = open(query_text_file, encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Q = tokenizer.texts_to_sequences([content])\n",
    "X_Q = pad_sequences(X_Q, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
    "X_Q[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now().timestamp()\n",
    "output = new_model.predict(X_Q) \n",
    "similarity_array = cosine_similarity(output, X_feature)[0]\n",
    " \n",
    "#Get top K indices \n",
    "indices = similarity_array.argsort()[-10:][::-1]\n",
    "end_time =  datetime.datetime.now().timestamp()\n",
    "\n",
    "total_retrieval_time = end_time - start_time #In Seconds\n",
    "total_retrieval_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filenames[indices])\n",
    "similarity_array[similarity_array.argsort()[-10:][::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
